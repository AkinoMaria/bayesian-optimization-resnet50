{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d879e95f-470d-460c-ae79-0c879be78f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19476\\3924851613.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.test.is_gpu_available()\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5d6ab0-3d99-4453-87b0-2f01d6efe134",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearSegmentedColormap\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedKFold\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\utils\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\sklearn\\utils\\_joblib.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m     _warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# joblib imports may raise DeprecationWarning on certain Python\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# versions\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m         Memory,\n\u001b[0;32m     12\u001b[0m         Parallel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         register_parallel_backend,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     26\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_parallel_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\__init__.py:129\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_pickle\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_compressor\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpu_count\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\parallel.py:31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger, short_format_time\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m memstr_to_bytes\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parallel_backends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (FallbackToBackend, MultiprocessingBackend,\n\u001b[0;32m     32\u001b[0m                                  ThreadingBackend, SequentialBackend,\n\u001b[0;32m     33\u001b[0m                                  LokyBackend)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eval_expr, _Sentinel\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Make sure that those two classes are part of the public joblib.parallel API\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# so that 3rd party backend implementers can import them from here.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\_parallel_backends.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABCMeta, abstractmethod\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     _TracebackCapturingWrapper,\n\u001b[0;32m     14\u001b[0m     _retrieve_traceback_capturing_wrapped_call\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiprocessing_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mp\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\_utils.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multiprocessing_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mp\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloky\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ExceptionWithTraceback\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# supported operators\u001b[39;00m\n\u001b[0;32m     15\u001b[0m operators \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     ast\u001b[38;5;241m.\u001b[39mAdd: op\u001b[38;5;241m.\u001b[39madd,\n\u001b[0;32m     17\u001b[0m     ast\u001b[38;5;241m.\u001b[39mSub: op\u001b[38;5;241m.\u001b[39msub,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     ast\u001b[38;5;241m.\u001b[39mUSub: op\u001b[38;5;241m.\u001b[39mneg,\n\u001b[0;32m     24\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\joblib\\externals\\loky\\__init__.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Future\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpu_count\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_loky_pickler\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreusable_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_reusable_executor\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloudpickle_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrap_non_picklable_objects\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:982\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1395\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1555\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:156\u001b[0m, in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:148\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.layers import (Input, Activation, Add, Dense, Conv2D,\n",
    "                                     GlobalAveragePooling2D, MaxPooling2D,\n",
    "                                     Dropout, BatchNormalization)\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d524d15",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f27c9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the path where our dataset is stored\n",
    "dataset_path = r\"C:\\Users\\User\\Desktop\\UMS\\FYP\\FYP 1\\dataset\\archive\\garbage-dataset\"\n",
    "\n",
    "# Retrieve the names of all folders (representing trash types) within the dataset directory\n",
    "garbage_types = os.listdir(dataset_path)\n",
    "\n",
    "# Set to store unique image dimensions for the entire dataset\n",
    "all_dimensions_set = set()\n",
    "\n",
    "# Iterate over each trash type (folder) to process images\n",
    "for garbage_type in garbage_types:\n",
    "    folder_path = os.path.join(dataset_path, garbage_type)\n",
    "    \n",
    "    # Verify that the current item is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'jpeg'))]\n",
    "        \n",
    "        # Display the count of images in the current folder\n",
    "        num_images = len(image_files)\n",
    "        print(f\"{garbage_type} folder contains {num_images} images.\")\n",
    "        \n",
    "        # Loop over each image to check its dimensions\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            with Image.open(image_path) as img:\n",
    "                # Extract the width, height, and channels (color depth) of the image and add to the dimensions set\n",
    "                width, height = img.size\n",
    "                channels = len(img.getbands())\n",
    "                all_dimensions_set.add((width, height, channels))\n",
    "                \n",
    "# Determine if all images in the entire dataset have the same dimensions \n",
    "if len(all_dimensions_set) == 1: \n",
    "    width, height, channel = all_dimensions_set.pop()\n",
    "    print(f\"\\nAll images in the dataset have the same dimensions: {width}x{height} with {channels} color channels.\")\n",
    "else:\n",
    "    print(\"\\nThe images in the dataset have different dimensions or color channels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb087674",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over each trash type (folder) to display images\n",
    "for garbage_type in garbage_types:\n",
    "    folder_path = os.path.join(dataset_path, garbage_type)\n",
    "    \n",
    "    # Verify that the current item is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'jpeg'))]\n",
    "        \n",
    "        # Select the first 10 images\n",
    "        image_files = image_files[:10]\n",
    "        \n",
    "        # Set up subplots\n",
    "        fig, axs = plt.subplots(1, 10, figsize=(15, 2))\n",
    "        \n",
    "        for i, image_file in enumerate(image_files):\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            with Image.open(image_path) as img:\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig.suptitle(garbage_type, fontsize=20, y=1.03)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e83a7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store image file paths and their respective labels\n",
    "data = []\n",
    "\n",
    "# Loop through each garbage type and collect its images' file paths\n",
    "for garbage_type in garbage_types:\n",
    "    for file in os.listdir(os.path.join(dataset_path, garbage_type)):\n",
    "        # Append the image file path and its trash type (as a label) to the data list\n",
    "        data.append((os.path.join(dataset_path, garbage_type, file), garbage_type))\n",
    "\n",
    "# Convert the collected data into a DataFrame\n",
    "df = pd.DataFrame(data, columns=['filepath', 'label'])\n",
    "\n",
    "# Display the first few entries of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eff730",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Split with stratification\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Print the number of images in each set\n",
    "print(f\"Number of images in the training set: {len(train_df)}\")\n",
    "print(f\"Number of images in the validation set: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e16c1-43c0-473e-9006-1727f7c9cd9c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. Class distribution in the entire dataset\n",
    "overall_distribution = df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# 2. Class distribution in the training set\n",
    "train_distribution = train_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# 3. Class distribution in the validation set\n",
    "val_distribution = val_df['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class distribution in the entire dataset:\\n\")\n",
    "print(overall_distribution.round(2))\n",
    "print('-'*40)\n",
    "\n",
    "print(\"\\nClass distribution in the training set:\\n\")\n",
    "print(train_distribution.round(2))\n",
    "print('-'*40)\n",
    "\n",
    "print(\"\\nClass distribution in the validation set:\\n\")\n",
    "print(val_distribution.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfa60e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute total number of images per class in the entire dataset\n",
    "overall_count = df['label'].value_counts()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(overall_count.index, overall_count, color='blue', alpha=0.7)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel(\"Class Labels\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Total Number of Images per Class in Dataset\")\n",
    "plt.xticks(rotation=45)  # Rotate labels for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the values on top of each bar\n",
    "for i, v in enumerate(overall_count):\n",
    "    plt.text(i, v + 5, str(v), ha='center', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b58e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight Augmentation settings for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,                     # Normalize pixel values to [0,1]\n",
    "    rotation_range=45,                  # Randomly rotate the images by up to 45 degrees\n",
    "    width_shift_range=0.15,             # Randomly shift images horizontally by up to 15% of the width\n",
    "    height_shift_range=0.15,            # Randomly shift images vertically by up to 15% of the height\n",
    "    zoom_range=0.15,                    # Randomly zoom in or out by up to 15%\n",
    "    horizontal_flip=True,               # Randomly flip images horizontally\n",
    "    vertical_flip=True,                 # Randomly flip images vertically\n",
    "    shear_range=0.05,                   # Apply slight shear transformations\n",
    "    brightness_range=[0.9, 1.1],        # Vary brightness between 90% to 110% of original\n",
    "    channel_shift_range=10,             # Randomly shift channels (can change colors of images slightly but less aggressively)\n",
    "    fill_mode='nearest'                 # Fill in missing pixels using the nearest filled value\n",
    ")\n",
    "\n",
    "# Only rescaling for validation\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ce572",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_types = len(garbage_types)\n",
    "\n",
    "# Set up the plot\n",
    "fig, axs = plt.subplots(num_types, 3, figsize=(15, 5 * num_types))\n",
    "\n",
    "for i, garbage_type in enumerate(garbage_types):\n",
    "    folder_path = os.path.join(dataset_path, garbage_type)\n",
    "    \n",
    "    # Verify that the current item is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'jpeg'))]\n",
    "        \n",
    "        if not image_files:\n",
    "            continue\n",
    "        \n",
    "        # Select one image\n",
    "        image_file = image_files[0]\n",
    "        \n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        sample_image = load_img(image_path)\n",
    "        sample_image_array = img_to_array(sample_image)\n",
    "        sample_image_array = np.expand_dims(sample_image_array, axis=0)  # Expand dims to match (1, height, width, channels)\n",
    "\n",
    "        # Generate a batch of augmented images\n",
    "        augmented_images = train_datagen.flow(sample_image_array, batch_size=1)\n",
    "        \n",
    "        # Display folder name on the left side\n",
    "        axs[i, 0].text(0.5, 0.5, garbage_type, fontsize=18, ha='center', va='center')\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        # Display original image in the first column\n",
    "        axs[i, 1].imshow(sample_image_array[0].astype('uint8'))\n",
    "        axs[i, 1].axis('off')\n",
    "        axs[i, 1].set_title('Original')\n",
    "        \n",
    "        # Display augmented image in the second column\n",
    "        batch = next(augmented_images)\n",
    "        image = batch[0]  # Take the first (and only) image in the batch\n",
    "        axs[i, 2].imshow(image)\n",
    "        axs[i, 2].axis('off')\n",
    "        axs[i, 2].set_title('Augmented')\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749bf9f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Using flow_from_dataframe to generate batches\n",
    "# Generate training batches from the training dataframe\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,                  # DataFrame containing training data\n",
    "    x_col=\"filepath\",                    # Column with paths to image files\n",
    "    y_col=\"label\",                       # Column with image labels\n",
    "    target_size=(224, 224),              # Resize all images to size of 224x224\n",
    "    batch_size=4,                       # Number of images per batch\n",
    "    class_mode='categorical',            # One-hot encode labels\n",
    "    seed=42,                             # Seed for random number generator to ensure reproducibility\n",
    "    shuffle=False                        # Data is not shuffled; order retained from DataFrame\n",
    ")\n",
    "\n",
    "\n",
    "# Generate validation batches from the validation dataframe\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,                    # DataFrame containing validation data\n",
    "    x_col=\"filepath\",                    # Column with paths to image files\n",
    "    y_col=\"label\",                       # Column with image labels\n",
    "    target_size=(224, 224),              # Resize all images to size of 224x224\n",
    "    batch_size=32,                       # Number of images per batch\n",
    "    class_mode='categorical',            # One-hot encode labels\n",
    "    seed=42,                             # Seed for random number generator to ensure reproducibility\n",
    "    shuffle=False                        # Data is not shuffled; order retained from DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74633c3c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of batches in train_generator: {len(train_generator)}\")\n",
    "print(f\"Number of batches in val_generator: {len(val_generator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e665b9cf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to count total number of images generated by a generator\n",
    "def count_images_in_generator(generator):\n",
    "    total_images = 0\n",
    "    for _ in generator:\n",
    "        total_images += generator.batch_size\n",
    "        if generator.batch_index == 0:\n",
    "            break  # To break out of infinite loop when all batches have been seen\n",
    "    return total_images\n",
    "\n",
    "# Count total augmented images\n",
    "total_train_images = count_images_in_generator(train_generator)\n",
    "total_val_images = count_images_in_generator(val_generator)\n",
    "\n",
    "print(f\"Total number of augmented images in training set: {total_train_images}\")\n",
    "print(f\"Total number of augmented images in validation set: {total_val_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b089ec6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Extract class labels from the 'label' column of train_df\n",
    "class_labels = train_df['label'].unique()\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d5f1aa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90003ba0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=train_df['label'])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4a6f8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert the computed weights to a dictionary for passing to model training\n",
    "class_weights = dict(zip(train_generator.class_indices.values(), weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5fdab4",
   "metadata": {},
   "source": [
    "# Building Resnet50 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df33f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(X, kernel_size, filters, reduce=False, stride=2):\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. We will need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    if reduce:\n",
    "        # If we are to reduce the spatial size, apply a 1x1 CONV layer to the shortcut path\n",
    "        X = Conv2D(filters = F1, \n",
    "                   kernel_size = (1, 1), \n",
    "                   strides = (stride, stride), \n",
    "                   padding = 'valid', \n",
    "                   kernel_initializer='he_normal')(X)\n",
    "        X = BatchNormalization(axis = 3)(X)\n",
    "        X = Activation('relu')(X)\n",
    "        \n",
    "        X_shortcut = Conv2D(filters = F3, \n",
    "                            kernel_size = (1, 1), \n",
    "                            strides = (stride, stride), \n",
    "                            padding = 'valid', \n",
    "                            kernel_initializer='he_normal')(X_shortcut)\n",
    "        X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n",
    "    else: \n",
    "        # First component of main path\n",
    "        X = Conv2D(filters = F1, \n",
    "                   kernel_size = (1, 1), \n",
    "                   strides = (1, 1), \n",
    "                   padding = 'valid', \n",
    "                   kernel_initializer='he_normal')(X)\n",
    "        X = BatchNormalization(axis = 3)(X)\n",
    "        X = Activation('relu')(X)\n",
    "    \n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters = F2, \n",
    "               kernel_size = (kernel_size, kernel_size), \n",
    "               strides = (1, 1), \n",
    "               padding = 'same', \n",
    "               kernel_initializer='he_normal')(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters = F3, \n",
    "               kernel_size = (1, 1), \n",
    "               strides = (1, 1), \n",
    "               padding = 'valid', \n",
    "               kernel_initializer='he_normal')(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a ReLU activation \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2304a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ResNet_50(input_shape, classes):\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), kernel_initializer='he_normal')(X_input)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Block 2\n",
    "    X = residual_block(X, 3, [64, 64, 256], reduce=True, stride=1)\n",
    "    X = residual_block(X, 3, [64, 64, 256])\n",
    "    X = residual_block(X, 3, [64, 64, 256])\n",
    "\n",
    "    # Block 3 \n",
    "    X = residual_block(X, 3, [128, 128, 512], reduce=True, stride=2)\n",
    "    X = residual_block(X, 3, [128, 128, 512])\n",
    "    X = residual_block(X, 3, [128, 128, 512])\n",
    "    X = residual_block(X, 3, [128, 128, 512])\n",
    "\n",
    "    # Block 4 \n",
    "    X = residual_block(X, 3, [256, 256, 1024], reduce=True, stride=2)\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "\n",
    "    # Block 5 \n",
    "    X = residual_block(X, 3, [512, 512, 2048], reduce=True, stride=2)\n",
    "    X = residual_block(X, 3, [512, 512, 2048])\n",
    "    X = residual_block(X, 3, [512, 512, 2048])\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimensions\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    \n",
    "    # Fully Connected Layer for classification\n",
    "    X = Dense(classes, activation='softmax')(X)\n",
    "        \n",
    "    # Create the model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a172edf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Modified_ResNet50(input_shape, classes):\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), kernel_initializer='he_normal')(X_input)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = residual_block(X, 3, [64, 64, 256], reduce=True, stride=1)\n",
    "    X = residual_block(X, 3, [64, 64, 256])\n",
    "    X = residual_block(X, 3, [64, 64, 256])\n",
    "\n",
    "    # Stage 3 \n",
    "    X = residual_block(X, 3, [128, 128, 512], reduce=True, stride=2)\n",
    "    X = residual_block(X, 3, [128, 128, 512])\n",
    "    X = residual_block(X, 3, [128, 128, 512])\n",
    "    X = residual_block(X, 3, [128, 128, 512])\n",
    "\n",
    "    # Stage 4 \n",
    "    X = residual_block(X, 3, [256, 256, 1024], reduce=True, stride=2)\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "    X = residual_block(X, 3, [256, 256, 1024])\n",
    "\n",
    "    # Stage 5 \n",
    "    X = residual_block(X, 3, [512, 512, 2048], reduce=True, stride=2)\n",
    "    X = residual_block(X, 3, [512, 512, 2048])\n",
    "    X = residual_block(X, 3, [512, 512, 2048])\n",
    "\n",
    "    # Global Average Pooling to reduce spatial dimensions\n",
    "    X = GlobalAveragePooling2D()(X)\n",
    "    \n",
    "    # Add Dropout to prevent overfitting\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    # Fully Connected Layer for classification\n",
    "    X = Dense(classes, activation='softmax')(X)\n",
    "        \n",
    "    # Create the model\n",
    "    model = Model(inputs = X_input, outputs = X, name='Modified_ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34626e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the shape of the input images and number of classes\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 5\n",
    "\n",
    "# Initialize the modified ResNet50 model with the specified parameters\n",
    "modified_resnet50_model = Modified_ResNet50(input_shape=input_shape, classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308dc1c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(modified_resnet50_model, show_shapes=True, show_layer_names=False, dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034c522",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modified_resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946d7e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "modified_resnet50_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b566f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=0.00001)\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6004dda",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of batches in train_generator: {len(train_generator)}\")\n",
    "print(f\"Number of batches in val_generator: {len(val_generator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775937d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Total number of epochs\n",
    "num_epochs = 200 \n",
    "\n",
    "# Train the model\n",
    "history = modified_resnet50_model.fit(train_generator, \n",
    "                                      steps_per_epoch=len(train_generator), \n",
    "                                      epochs=num_epochs, \n",
    "                                      validation_data=val_generator, \n",
    "                                      validation_steps=len(val_generator),\n",
    "                                      class_weight=class_weights,\n",
    "                                      callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4346c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, start_epoch=5):\n",
    "\n",
    "    # Convert the history.history dict to a pandas DataFrame\n",
    "    df = pd.DataFrame(history.history)\n",
    "\n",
    "    # Plot the curves from the specified epoch onwards\n",
    "    df = df.iloc[start_epoch-1:]\n",
    "\n",
    "    # Set the style of seaborn for better visualization\n",
    "    sns.set(rc={'axes.facecolor': '#f0f0fc'}, style='darkgrid')\n",
    "\n",
    "    # Plotting the learning curves\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.lineplot(x=df.index, y=df['loss'], color='royalblue', label='Train Loss')\n",
    "    sns.lineplot(x=df.index, y=df['val_loss'], color='orangered', linestyle='--', label='Validation Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "\n",
    "    # Plotting the training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.lineplot(x=df.index, y=df['accuracy'], color='royalblue', label='Train Accuracy')\n",
    "    sns.lineplot(x=df.index, y=df['val_accuracy'], color='orangered', linestyle='--', label='Validation Accuracy')\n",
    "    plt.title('Accuracy Evolution')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c1a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac99464",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, val_generator, class_labels):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance on the validation set and print the classification report.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model.\n",
    "    - val_generator: Validation data generator.\n",
    "    - class_labels: List of class names.\n",
    "    \n",
    "    Returns:\n",
    "    - report: Classification report as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting all the true labels for the validation set\n",
    "    true_labels = val_generator.classes\n",
    "\n",
    "    # Get the class labels (names) from the generator\n",
    "    class_labels = list(val_generator.class_indices.keys())\n",
    "\n",
    "    # To get the predicted labels, we predict using the model  \n",
    "    predictions = model.predict(val_generator, steps=len(val_generator))\n",
    "    \n",
    "    # Take the argmax to get the predicted class indices.\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Extracting true labels from the validation generator\n",
    "    true_labels = val_generator.classes\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(true_labels, predicted_labels, target_names=class_labels)\n",
    "    print(report)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Define a custom colormap\n",
    "    colors = [\"white\", \"royalblue\"]\n",
    "    cmap_cm = LinearSegmentedColormap.from_list(\"cmap_cm\", colors)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Plotting confusion matrix using seaborn\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, cmap=cmap_cm, fmt='d', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96de55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(modified_resnet50_model, val_generator, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model with weights pre-trained on ImageNet\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab154fac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f8a20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(base_model.layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0787d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    if 140 <= i <= 150:\n",
    "        print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers up to conv4_block6_out\n",
    "for layer in base_model.layers[:143]: # include the layer 142\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec355c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new model with transfer learning\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(5, activation='softmax')(x)\n",
    "\n",
    "transfer_resnet50_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "transfer_resnet50_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ff368",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(transfer_resnet50_model, show_shapes=True, show_layer_names=False, dpi=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ec360",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transfer_resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight Augmentation settings for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=60,                  # Randomly rotate the images by up to 60 degrees\n",
    "    width_shift_range=0.15,             # Randomly shift images horizontally by up to 15% of the width\n",
    "    height_shift_range=0.15,            # Randomly shift images vertically by up to 15% of the height\n",
    "    zoom_range=0.20,                    # Randomly zoom in or out by up to 20%\n",
    "    horizontal_flip=True,               # Randomly flip images horizontally\n",
    "    vertical_flip=True,                 # Randomly flip images vertically\n",
    "    shear_range=0.05,                   # Apply slight shear transformations\n",
    "    brightness_range=[0.9, 1.1],        # Vary brightness between 90% to 110% of original\n",
    "    channel_shift_range=10,             # Randomly shift channels (can change colors of images slightly but less aggressively)\n",
    "    fill_mode='nearest',                 # Fill in missing pixels using the nearest filled value\n",
    "    preprocessing_function=preprocess_input  # Add this line\n",
    ")\n",
    "\n",
    "# For the validation set, you might not have augmentation:\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using flow_from_dataframe to generate batches\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,                  # DataFrame containing training data\n",
    "    x_col=\"filepath\",                    # Column with paths to image files\n",
    "    y_col=\"label\",                       # Column with image labels\n",
    "    target_size=(224, 224),              # Resize all images to size of 224x224\n",
    "    batch_size=32,                       # Number of images per batch\n",
    "    class_mode='categorical',            # One-hot encode labels\n",
    "    seed=42,                             # Seed for random number generator to ensure reproducibility\n",
    "    shuffle=False                        # Data is not shuffled; order retained from DataFrame\n",
    ")\n",
    "\n",
    "\n",
    "# Generate validation batches from the validation dataframe\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,                    # DataFrame containing validation data\n",
    "    x_col=\"filepath\",                    # Column with paths to image files\n",
    "    y_col=\"label\",                       # Column with image labels\n",
    "    target_size=(224, 224),              # Resize all images to size of 224x224\n",
    "    batch_size=32,                       # Number of images per batch\n",
    "    class_mode='categorical',            # One-hot encode labels\n",
    "    seed=42,                             # Seed for random number generator to ensure reproducibility\n",
    "    shuffle=False                        # Data is not shuffled; order retained from DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3850b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=15, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Train the baseline transfer_resnet50_model\n",
    "baseline_history = transfer_resnet50_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    epochs=50,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save baseline metrics\n",
    "baseline_val_loss = min(baseline_history.history['val_loss'])\n",
    "baseline_val_accuracy = max(baseline_history.history['val_accuracy'])\n",
    "print(f\"Baseline Model - Val Loss: {baseline_val_loss}, Val Accuracy: {baseline_val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(baseline_history, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ee3d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_performance(transfer_resnet50_model, val_generator, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8204569",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load your custom model (Replace with your model loading code)\n",
    "model = transfer_resnet50_model\n",
    "\n",
    "# Function to load and preprocess a random image from a folder\n",
    "def load_random_image_from_folder(folder_path):\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.endswith(('jpg', 'jpeg', 'png'))\n",
    "    ]\n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(f\"No valid image files in folder: {folder_path}\")\n",
    "    \n",
    "    random_image_file = random.choice(image_files)\n",
    "    image_path = os.path.join(folder_path, random_image_file)\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_resized = img.resize((224, 224))\n",
    "    img_array = img_to_array(img_resized)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    return img, img_array\n",
    "\n",
    "# Function to predict and visualize one image per class\n",
    "def predict_and_plot_each_class(dataset_path):\n",
    "    class_labels = sorted([\n",
    "        folder for folder in os.listdir(dataset_path)\n",
    "        if os.path.isdir(os.path.join(dataset_path, folder))\n",
    "    ])\n",
    "    \n",
    "    num_classes = len(class_labels)\n",
    "    if num_classes == 0:\n",
    "        raise ValueError(\"No class folders found in the dataset path.\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, num_classes, figsize=(15, 5))\n",
    "    fig.suptitle(\"Prediction Results for One Image Per Class\", fontsize=16)\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        folder_path = os.path.join(dataset_path, class_label)\n",
    "        \n",
    "        try:\n",
    "            img, img_array = load_random_image_from_folder(folder_path)\n",
    "            predictions = model.predict(img_array)\n",
    "            predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "            predicted_label = class_labels[predicted_class_index]\n",
    "            predicted_confidence = predictions[0][predicted_class_index]\n",
    "            \n",
    "            axs[i].imshow(img)\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(\n",
    "                f\"Predicted: {predicted_label}\\n\"\n",
    "                f\"Confidence: {predicted_confidence*100:.2f}%\\n\"\n",
    "                f\"True Label: {class_label}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(f\"Error: {e}\")\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "    plt.show()\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = r\"C:\\Users\\User\\Desktop\\UMS\\FYP\\FYP 1\\dataset\\TrashType_Image_Dataset\"\n",
    "\n",
    "# Call the function\n",
    "predict_and_plot_each_class(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4b5fd-d205-4180-ba03-223992a37fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Define the search space\n",
    "search_space = [\n",
    "    Real(1e-5, 1e-2, name='learning_rate', prior='log-uniform'),\n",
    "    Real(0.1, 0.6, name='dropout'),\n",
    "    Integer(16, 64, name='batch_size'),\n",
    "]\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def objective(**params):\n",
    "    learning_rate = params['learning_rate']\n",
    "    dropout = params['dropout']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    # Create a new model\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=len(val_generator),\n",
    "        epochs=10,  # Fewer epochs for faster optimization\n",
    "        batch_size=batch_size,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=[reduce_lr, early_stopping],\n",
    "        verbose=0  # Suppress detailed output for optimization\n",
    "    )\n",
    "    \n",
    "    # Validation loss is the metric to minimize\n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    return val_loss\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "result = gp_minimize(\n",
    "    func=objective,\n",
    "    dimensions=search_space,\n",
    "    n_calls=30,  # Number of evaluations\n",
    "    n_initial_points=5,  # Number of random initial points\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Extract the best parameters\n",
    "best_params = {dim.name: val for dim, val in zip(search_space, result.x)}\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef958e0-2e59-43e6-9fe9-548c485831bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b93fab-0111-447e-bb43-e53d8f18bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new model with transfer learning\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(5, activation='softmax')(x)\n",
    "\n",
    "optimized_resnet50_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "optimized_resnet50_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-05), \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c7d71-d441-4988-b3cf-6eeccc6eb46b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Slight Augmentation settings for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=60,                  # Randomly rotate the images by up to 60 degrees\n",
    "    width_shift_range=0.15,             # Randomly shift images horizontally by up to 15% of the width\n",
    "    height_shift_range=0.15,            # Randomly shift images vertically by up to 15% of the height\n",
    "    zoom_range=0.20,                    # Randomly zoom in or out by up to 20%\n",
    "    horizontal_flip=True,               # Randomly flip images horizontally\n",
    "    vertical_flip=True,                 # Randomly flip images vertically\n",
    "    shear_range=0.05,                   # Apply slight shear transformations\n",
    "    brightness_range=[0.9, 1.1],        # Vary brightness between 90% to 110% of original\n",
    "    channel_shift_range=10,             # Randomly shift channels (can change colors of images slightly but less aggressively)\n",
    "    fill_mode='nearest',                 # Fill in missing pixels using the nearest filled value\n",
    "    preprocessing_function=preprocess_input  # Add this line\n",
    ")\n",
    "\n",
    "# For the validation set, you might not have augmentation:\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  # Add this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936c172-6916-4da3-aa97-c6394727234a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Using flow_from_dataframe to generate batches\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,                  # DataFrame containing training data\n",
    "    x_col=\"filepath\",                    # Column with paths to image files\n",
    "    y_col=\"label\",                       # Column with image labels\n",
    "    target_size=(224, 224),              # Resize all images to size of 224x224\n",
    "    batch_size=32,                       # Number of images per batch\n",
    "    class_mode='categorical',            # One-hot encode labels\n",
    "    seed=42,                             # Seed for random number generator to ensure reproducibility\n",
    "    shuffle=False                        # Data is not shuffled; order retained from DataFrame\n",
    ")\n",
    "\n",
    "\n",
    "# Generate validation batches from the validation dataframe\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,                    # DataFrame containing validation data\n",
    "    x_col=\"filepath\",                    # Column with paths to image files\n",
    "    y_col=\"label\",                       # Column with image labels\n",
    "    target_size=(224, 224),              # Resize all images to size of 224x224\n",
    "    batch_size=32,                       # Number of images per batch\n",
    "    class_mode='categorical',            # One-hot encode labels\n",
    "    seed=42,                             # Seed for random number generator to ensure reproducibility\n",
    "    shuffle=False                        # Data is not shuffled; order retained from DataFrame\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309ae68-3627-4702-bc87-3be99f236218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=15, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Total number of epochs\n",
    "num_epochs = 50  \n",
    "\n",
    "# Train the model\n",
    "history = optimized_resnet50_model.fit(train_generator,\n",
    "                                      steps_per_epoch=len(train_generator), \n",
    "                                      epochs=num_epochs,\n",
    "                                      validation_data=val_generator, \n",
    "                                      validation_steps=len(val_generator),\n",
    "                                      class_weight=class_weights,\n",
    "                                       batch_size=16,\n",
    "                                      callbacks=[reduce_lr, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95035e25-7c1b-4844-ab02-0d25f7ac78d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8943a7-e9f2-49a2-a0b9-9fbda8b70668",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance(optimized_resnet50_model, val_generator, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca8f08-7b3d-4582-83df-24627fbd2d9b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load your custom model (Replace with your model loading code)\n",
    "model = optimized_resnet50_model\n",
    "\n",
    "# Function to load and preprocess a random image from a folder\n",
    "def load_random_image_from_folder(folder_path):\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.endswith(('jpg', 'jpeg', 'png'))\n",
    "    ]\n",
    "    if not image_files:\n",
    "        raise FileNotFoundError(f\"No valid image files in folder: {folder_path}\")\n",
    "    \n",
    "    random_image_file = random.choice(image_files)\n",
    "    image_path = os.path.join(folder_path, random_image_file)\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_resized = img.resize((224, 224))\n",
    "    img_array = img_to_array(img_resized)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    return img, img_array\n",
    "\n",
    "# Function to predict and visualize one image per class\n",
    "def predict_and_plot_each_class(dataset_path):\n",
    "    class_labels = sorted([\n",
    "        folder for folder in os.listdir(dataset_path)\n",
    "        if os.path.isdir(os.path.join(dataset_path, folder))\n",
    "    ])\n",
    "    \n",
    "    num_classes = len(class_labels)\n",
    "    if num_classes == 0:\n",
    "        raise ValueError(\"No class folders found in the dataset path.\")\n",
    "    \n",
    "    fig, axs = plt.subplots(1, num_classes, figsize=(15, 5))\n",
    "    fig.suptitle(\"Prediction Results for One Image Per Class\", fontsize=16)\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        folder_path = os.path.join(dataset_path, class_label)\n",
    "        \n",
    "        try:\n",
    "            img, img_array = load_random_image_from_folder(folder_path)\n",
    "            predictions = model.predict(img_array)\n",
    "            predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
    "            predicted_label = class_labels[predicted_class_index]\n",
    "            predicted_confidence = predictions[0][predicted_class_index]\n",
    "            \n",
    "            axs[i].imshow(img)\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(\n",
    "                f\"Predicted: {predicted_label}\\n\"\n",
    "                f\"Confidence: {predicted_confidence*100:.2f}%\\n\"\n",
    "                f\"True Label: {class_label}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(f\"Error: {e}\")\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "    plt.show()\n",
    "\n",
    "# Define dataset path\n",
    "dataset_path = r\"C:\\Users\\User\\Desktop\\UMS\\FYP\\FYP 1\\dataset\\TrashType_Image_Dataset\"\n",
    "\n",
    "# Call the function\n",
    "predict_and_plot_each_class(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfea664-f155-4572-a9a1-3ab84cc80a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized metrics\n",
    "optimized_val_loss = min(history.history['val_loss'])\n",
    "optimized_val_accuracy = max(history.history['val_accuracy'])\n",
    "print(f\"Optimized Model - Val Loss: {optimized_val_loss}, Val Accuracy: {optimized_val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7139eb8-4b7a-49ee-8ef5-51136f6b0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of epochs for each model\n",
    "baseline_epochs = len(baseline_val_loss)\n",
    "optimized_epochs = len(optimized_val_loss)\n",
    "\n",
    "# Use the shorter epoch count to avoid mismatch\n",
    "min_epochs = min(baseline_epochs, optimized_epochs)\n",
    "epochs = range(1, min_epochs + 1)\n",
    "\n",
    "# Truncate validation loss and accuracy to match the shorter history\n",
    "baseline_val_loss = baseline_val_loss[:min_epochs]\n",
    "baseline_val_accuracy = baseline_val_accuracy[:min_epochs]\n",
    "optimized_val_loss = optimized_val_loss[:min_epochs]\n",
    "optimized_val_accuracy = optimized_val_accuracy[:min_epochs]\n",
    "\n",
    "# Plot Validation Loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, baseline_val_loss, label='Baseline - Val Loss', color='dodgerblue', marker='o')\n",
    "plt.plot(epochs, optimized_val_loss, label='Optimized - Val Loss', color='blue', marker='o')\n",
    "plt.title('Validation Loss Across Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, baseline_val_accuracy, label='Baseline - Val Accuracy', color='dodgerblue', marker='o')\n",
    "plt.plot(epochs, optimized_val_accuracy, label='Optimized - Val Accuracy', color='blue', marker='o')\n",
    "plt.title('Validation Accuracy Across Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1a140-ba25-4e71-8290-a65247aab5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "optimized_resnet50_results = evaluate_model_performance(optimized_resnet50_model, val_generator, class_labels)\n",
    "transfer_resnet50_results = evaluate_model_performance(transfer_resnet50_model, val_generator, class_labels)\n",
    "\n",
    "# Extract accuracy from results (Assuming results are in the form of a dictionary)\n",
    "optimized_resnet50_accuracy = optimized_resnet50_results['accuracy']  # Adjust if your results are different\n",
    "transfer_resnet50_accuracy = transfer_resnet50_results['accuracy']  # Adjust if your results are different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905d363-5ee1-4ece-af95-c92f888710d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar plot to compare the accuracies\n",
    "model_names = ['Optimized ResNet50', 'ResNet50']\n",
    "accuracies = [optimized_resnet50_accuracy, transfer_resnet50_accuracy]\n",
    "\n",
    "plt.bar(model_names, accuracies, color=['blue', 'green'])\n",
    "plt.title('Accuracy Comparison: Optimized vs Transfer ResNet50')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)  # Assuming accuracy is between 0 and 1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b9a479-4397-4e53-8cfe-cf849a4282c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
